<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Publications | Scott M. Jordan</title> <meta name="author" content="Scott M. Jordan"> <meta name="description" content="publications by categories in reversed chronological order."> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/UA_Logo_GreenRGB_crest.jpeg"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://scottjordan.github.io/publications/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Scott </span>M. Jordan</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description">publications by categories in reversed chronological order.</p> </header> <article> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://aaai.org/" rel="external nofollow noopener" target="_blank">AAAI</a></abbr></div> <div id="gupta2024bidirection" class="col-sm-8"> <div class="title">From Past to Future: Rethinking Eligibility Traces</div> <div class="author"> <a href="https://dhawgupta.com/" rel="external nofollow noopener" target="_blank">Dhawal Gupta</a>, Scott M Jordan, Shreyas Chaudhari, Bo Liu, Philip S Thomas, and <a href="https://people.cs.umass.edu/~bsilva/" rel="external nofollow noopener" target="_blank">Bruno Castro Silva</a> </div> <div class="periodical"> <em>In Proceedings of the AAAI Conference on Artificial Intelligence</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2312.12972" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>In this paper, we introduce a fresh perspective on the challenges of credit assignment and policy evaluation. First, we delve into the nuances of eligibility traces and explore instances where their updates may result in unexpected credit assignment to preceding states. From this investigation emerges the concept of a novel value function, which we refer to as the \emphbidirectional value function. Unlike traditional state value functions, bidirectional value functions account for both future expected returns (rewards anticipated from the current state onward) and past expected returns (cumulative rewards from the episode’s start to the present). We derive principled update equations to learn this value function and, through experimentation, demonstrate its efficacy in enhancing the process of policy evaluation. In particular, our results indicate that the proposed learning approach can, in certain challenging contexts, perform policy evaluation more rapidly than TD(λ) – a method that learns forward value functions, vπ, \emphdirectly. Overall, our findings present a new perspective on eligibility traces and potential advantages associated with the novel value function it inspires, especially for policy evaluation. </p> </div> </div> </div> </li></ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="DBLP:journals/corr/abs-2302-01248" class="col-sm-8"> <div class="title">Avoiding Model Estimation in Robust Markov Decision Processes with a Generative Model</div> <div class="author"> <a href="https://yangwenhaosms.github.io/" rel="external nofollow noopener" target="_blank">Wenhao Yang</a>, Han Wang, Tadashi Kozuno, <em>Scott M. Jordan</em>, and Zhihua Zhang</div> <div class="periodical"> <em>CoRR</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2302.01248" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="DBLP:journals/corr/abs-2305-09838" class="col-sm-8"> <div class="title">Coagent Networks: Generalized and Scaled</div> <div class="author"> <a href="https://people.cs.umass.edu/~jekostas/jekostas.html" rel="external nofollow noopener" target="_blank">James E. Kostas</a>, <em>Scott M. Jordan</em>, <a href="https://yashchandak.github.io/" rel="external nofollow noopener" target="_blank">Yash Chandak</a>, Georgios Theocharous, <a href="https://dhawgupta.com/" rel="external nofollow noopener" target="_blank">Dhawal Gupta</a>, <a href="https://webdocs.cs.ualberta.ca/~whitem/" rel="external nofollow noopener" target="_blank">Martha White</a>, <a href="https://people.cs.umass.edu/~bsilva/" rel="external nofollow noopener" target="_blank">Bruno Castro Silva</a>, and <a href="https://people.cs.umass.edu/~pthomas/" rel="external nofollow noopener" target="_blank">Philip S. Thomas</a> </div> <div class="periodical"> <em>CoRR</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2305.09838" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="jordan2023rigorous" class="col-sm-8"> <div class="title">Rigorous Experimentation For Reinforcement Learning</div> <div class="author"> <em>Scott M. Jordan</em> </div> <div class="periodical"> <em>University of Massachusetts Amherst</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://scholarworks.umass.edu/dissertations_2/2760/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://nips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS</a></abbr></div> <div id="gupta2023behavior" class="col-sm-8"> <div class="title">Behavior Alignment via Reward Function Optimization</div> <div class="author"> <a href="https://dhawgupta.com/" rel="external nofollow noopener" target="_blank">Dhawal Gupta</a>, <a href="https://yashchandak.github.io/" rel="external nofollow noopener" target="_blank">Yash Chandak</a>, Scott M Jordan, Philip S Thomas, and <a href="https://people.cs.umass.edu/~bsilva/" rel="external nofollow noopener" target="_blank">Bruno Castro Silva</a> </div> <div class="periodical"> <em>In Proceedings of the Thirty-seventh Conference on Neural Information Process Systems, NeurIPS</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2310.19007" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://neurips.cc/virtual/2023/poster/71752" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Designing reward functions for efficiently guiding reinforcement learning (RL) agents toward specific behaviors is a complex task. This is challenging since it requires the identification of reward structures that are not sparse and that avoid inadvertently inducing undesirable behaviors. Naively modifying the reward structure to offer denser and more frequent feedback can lead to unintended outcomes and promote behaviors that are not aligned with the designer’s intended goal. Although potential-based reward shaping is often suggested as a remedy, we systematically investigate settings where deploying it often significantly impairs performance. To address these issues, we introduce a new framework that uses a bi-level objective to learn \emphbehavior alignment reward functions. These functions integrate auxiliary rewards reflecting a designer’s heuristics and domain knowledge with the environment’s primary rewards. Our approach automatically determines the most effective way to blend these types of feedback, thereby enhancing robustness against heuristic reward misspecification. Remarkably, it can also adapt an agent’s policy optimization process to mitigate suboptimalities resulting from limitations and biases inherent in the underlying RL algorithms. We evaluate our method’s efficacy on a diverse set of tasks, from small-scale experiments to high-dimensional control challenges. We investigate heuristic auxiliary rewards of varying quality – some of which are beneficial and others detrimental to the learning process. Our results show that our framework offers a robust and principled way to integrate designer-specified heuristics. It not only addresses key shortcomings of existing approaches but also consistently leads to high-performing solutions, even when given misaligned or poorly-specified auxiliary reward functions. </p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="jordan2022scientific" class="col-sm-8"> <div class="title">Scientific Experimentation for Reinforcement Learning</div> <div class="author"> <em>Scott M. Jordan</em> </div> <div class="periodical"> <em>Opinion Talk - Deep Reinforcement Learning Workshop at NeurIPS</em>, Dec 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://slideslive.com/38994681/scientific-experiments-in-reinforcement-learning" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/Opinion_Talk_Scientific_Experiments.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/opiniontalk.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li></ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://icml.cc/" rel="external nofollow noopener" target="_blank">ICML</a></abbr></div> <div id="DBLP:conf/icml/KostasCJTT21" class="col-sm-8"> <div class="title">High Confidence Generalization for Reinforcement Learning</div> <div class="author"> <a href="https://people.cs.umass.edu/~jekostas/jekostas.html" rel="external nofollow noopener" target="_blank">James E. Kostas</a>, <a href="https://yashchandak.github.io/" rel="external nofollow noopener" target="_blank">Yash Chandak</a>, <em>Scott M. Jordan</em>, Georgios Theocharous, and <a href="https://people.cs.umass.edu/~pthomas/" rel="external nofollow noopener" target="_blank">Philip S. Thomas</a> </div> <div class="periodical"> <em>In ICML</em>, Dec 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.mlr.press/v139/kostas21a.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://people.cs.umass.edu/%C2%A0pthomas/papers/Kostas2021.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>We present several classes of reinforcement learning algorithms that safely generalize to Markov decision processes (MDPs) not seen during training. Specifically, we study the setting in which some set of MDPs is accessible for training. The goal is to generalize safely to MDPs that are sampled from the same distribution, but which may not be in the set accessible for training. For various definitions of safety, our algorithms give probabilistic guarantees that agents can safely generalize to MDPs that are sampled from the same distribution but are not necessarily in the training set. These algorithms are a type of Seldonian algorithm (Thomas et al., 2019), which is a class of machine learning algorithms that return models with probabilistic safety guarantees for user-specified definitions of safety.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="bartlett2021impact" class="col-sm-8"> <div class="title">Impact of changes in tissue optical properties on near-infrared diffuse correlation spectroscopy measures of skeletal muscle blood flow</div> <div class="author"> Miles F Bartlett, <em>Scott M. Jordan</em>, Dennis M Hueber, and Michael D Nelson</div> <div class="periodical"> <em>Journal of Applied Physiology</em>, Dec 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://journals.physiology.org/doi/full/10.1152/japplphysiol.00857.2020" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/ScottJordan/dcs_bfi_fitting" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Near-infrared diffuse correlation spectroscopy (DCS) is increasingly used to study relative changes in skeletal muscle blood flow. However, most diffuse correlation spectrometers assume that tissue optical properties-such as absorption (\mu_a) and reduced scattering (\mu_s’) coefficients-remain constant during physiological provocations, which is untrue for skeletal muscle. Here, we interrogate how changes in tissue \mu_a and \mu_s’ affect DCS calculations of blood flow index (BFI). We recalculated BFI using raw autocorrelation curves and \mu_a/\mu_s’ values recorded during a reactive hyperemia protocol in 16 healthy young individuals. First, we show that incorrectly assuming baseline \mu_a and \mu_s’ substantially affects peak BFI and BFI slope when expressed in absolute terms (cm2/s, P &lt; 0.01), but these differences are abolished when expressed in relative terms (% baseline). Next, to evaluate the impact of physiologic changes in \mu_a and \mu_s’, we compared peak BFI and BFI slope when \mu_a and \mu_s’ were held constant throughout the reactive hyperemia protocol versus integrated from a 3-s rolling average. Regardless of approach, group means for peak BFI and BFI slope did not differ. Group means for peak BFI and BFI slope were also similar following ad absurdum analyses, where we simulated supraphysiologic changes in \mu_a/\mu_s’. In both cases, however, we identified individual cases where peak BFI and BFI slope were indeed affected, with this result being driven by relative changes in \mu_a over \mu_s’. Overall, these results provide support for past reports in which \mu_a/\mu_s’ were held constant but also advocate for real-time incorporation of \mu_a and \mu_s’ moving forward. NEW &amp; NOTEWORTHY We investigated how changes in tissue optical properties affect near-infrared diffuse correlation spectroscopy (NIR-DCS)-derived indices of skeletal muscle blood flow (BFI) during physiological provocation. Although accounting for changes in tissue optical properties has little impact on BFI on a group level, individual BFI calculations are indeed impacted by changes in tissue optical properties. NIR-DCS calculations of BFI should therefore account for real-time, physiologically induced changes in tissue optical properties whenever possible. </p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://icml.cc/" rel="external nofollow noopener" target="_blank">ICML</a></abbr></div> <div id="DBLP:conf/icml/JordanCCZT20" class="col-sm-8"> <div class="title">Evaluating the Performance of Reinforcement Learning Algorithms</div> <div class="author"> <em>Scott M. Jordan</em>, <a href="https://yashchandak.github.io/" rel="external nofollow noopener" target="_blank">Yash Chandak</a>, <a href="https://dscohen.github.io/" rel="external nofollow noopener" target="_blank">Daniel Cohen</a>, Mengxue Zhang, and <a href="https://people.cs.umass.edu/~pthomas/" rel="external nofollow noopener" target="_blank">Philip S. Thomas</a> </div> <div class="periodical"> <em>In ICML</em>, Dec 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2006.16958" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="http://proceedings.mlr.press/v119/jordan20a.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/ScottJordan/EvaluationOfRLAlgs" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://icml.cc/media/icml-2020/Slides/6301.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Slides</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Performance evaluations are critical for quantifying algorithmic advances in reinforcement learning. Recent reproducibility analyses have shown that reported performance results are often inconsistent and difficult to replicate. In this work, we argue that the inconsistency of performance stems from the use of flawed evaluation metrics. Taking a step towards ensuring that reported results are consistent, we propose a new comprehensive evaluation methodology for reinforcement learning algorithms that produces reliable measurements of performance both on a single environment and when aggregated across environments. We demonstrate this method by evaluating a broad class of reinforcement learning algorithms on standard benchmark tasks. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://nips.cc/" rel="external nofollow noopener" target="_blank">NeurIPS</a></abbr></div> <div id="DBLP:conf/nips/ChandakJTWT20" class="col-sm-8"> <div class="title">Towards Safe Policy Improvement for Non-Stationary MDPs</div> <div class="author"> <a href="https://yashchandak.github.io/" rel="external nofollow noopener" target="_blank">Yash Chandak</a>, <em>Scott M. Jordan</em>, Georgios Theocharous, <a href="https://webdocs.cs.ualberta.ca/~whitem/" rel="external nofollow noopener" target="_blank">Martha White</a>, and <a href="https://people.cs.umass.edu/~pthomas/" rel="external nofollow noopener" target="_blank">Philip S. Thomas</a> </div> <div class="periodical"> <em>In NeurIPS</em>, Dec 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2010.12645" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://proceedings.neurips.cc//paper_files/paper/2020/hash/680390c55bbd9ce416d1d69a9ab4760d-Abstract.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/ScottJordan/SafePolicyImprovementNonstationary" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Many real-world sequential decision-making problems involve critical systems with financial risks and human-life risks. While several works in the past have proposed methods that are safe for deployment, they assume that the underlying problem is stationary. However, many real-world problems of interest exhibit non-stationarity, and when stakes are high, the cost associated with a false stationarity assumption may be unacceptable. We take the first steps towards ensuring safety, with high confidence, for smoothly-varying non-stationary decision problems. Our proposed method extends a type of safe algorithm, called a Seldonian algorithm, through a synthesis of model-free reinforcement learning with time-series analysis. Safety is ensured using sequential hypothesis testing of a policy’s forecasted performance, and confidence intervals are obtained using wild bootstrap.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge"><a href="https://icml.cc/" rel="external nofollow noopener" target="_blank">ICML</a></abbr></div> <div id="DBLP:conf/icml/ChandakTKJT19" class="col-sm-8"> <div class="title">Learning Action Representations for Reinforcement Learning</div> <div class="author"> <a href="https://yashchandak.github.io/" rel="external nofollow noopener" target="_blank">Yash Chandak</a>, Georgios Theocharous, <a href="https://people.cs.umass.edu/~jekostas/jekostas.html" rel="external nofollow noopener" target="_blank">James E. Kostas</a>, <em>Scott M. Jordan</em>, and <a href="https://people.cs.umass.edu/~pthomas/" rel="external nofollow noopener" target="_blank">Philip S. Thomas</a> </div> <div class="periodical"> <em>In ICML</em>, Dec 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1902.00183" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="http://proceedings.mlr.press/v97/chandak19a.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Most model-free reinforcement learning methods leverage state representations (embeddings) for generalization, but either ignore structure in the space of actions or assume the structure is provided a priori. We show how a policy can be decomposed into a component that acts in a low-dimensional space of action representations and a component that transforms these representations into actual actions. These representations improve generalization over large, finite action sets by allowing the agent to infer the outcomes of actions similar to actions already taken. We provide an algorithm to both learn and use action representations and provide conditions for its convergence. The efficacy of the proposed method is demonstrated on large-scale real-world problems. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICTIR</abbr></div> <div id="DBLP:conf/ictir/CohenJC19" class="col-sm-8"> <div class="title">Learning a Better Negative Sampling Policy with Deep Neural Networks for Search</div> <div class="author"> <a href="https://dscohen.github.io/" rel="external nofollow noopener" target="_blank">Daniel Cohen</a>, <em>Scott M. Jordan</em>, and W. Bruce Croft</div> <div class="periodical"> <em>In ICTIR</em>, Dec 2019 </div> <div class="periodical"> </div> <div class="links"> <a href="https://ciir-publications.cs.umass.edu/getpdf.php?id=1337" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="DBLP:journals/corr/abs-1906-03063" class="col-sm-8"> <div class="title">Classical Policy Gradient: Preserving Bellman’s Principle of Optimality</div> <div class="author"> <a href="https://people.cs.umass.edu/~pthomas/" rel="external nofollow noopener" target="_blank">Philip S. Thomas</a>, <em>Scott M. Jordan</em>, <a href="https://yashchandak.github.io/" rel="external nofollow noopener" target="_blank">Yash Chandak</a>, Chris Nota, and <a href="https://people.cs.umass.edu/~jekostas/jekostas.html" rel="external nofollow noopener" target="_blank">James E. Kostas</a> </div> <div class="periodical"> <em>CoRR</em>, Dec 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1906.03063" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>We propose a new objective function for finite-horizon episodic Markov decision processes that better captures Bellman’s principle of optimality, and provide an expression for the gradient of the objective.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="jordan2019rldm" class="col-sm-8"> <div class="title">Evaluating Reinforcement Learning Algorithms Using Cumulative Distributions of Performance</div> <div class="author"> <em>Scott M. Jordan</em>, <a href="https://yashchandak.github.io/" rel="external nofollow noopener" target="_blank">Yash Chandak</a>, Mengxue Zhang, <a href="https://dscohen.github.io/" rel="external nofollow noopener" target="_blank">Daniel Cohen</a>, and <a href="https://people.cs.umass.edu/~pthomas/" rel="external nofollow noopener" target="_blank">Philip S. Thomas</a> </div> <div class="periodical"> <em>Fourth Multidisciplinary Conference on Reinforcement Learning and Decision Making (RLDM)</em>, Jul 2019 </div> <div class="periodical"> </div> <div class="links"> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> </div> </div> </li> </ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="DBLP:journals/corr/abs-1806-03790" class="col-sm-8"> <div class="title">Distributed Evaluations: Ending Neural Point Metrics</div> <div class="author"> <a href="https://dscohen.github.io/" rel="external nofollow noopener" target="_blank">Daniel Cohen</a>, <em>Scott M. Jordan</em>, and W. Bruce Croft</div> <div class="periodical"> <em>In ACM SIGIR - LND4IR Workshop</em>, Jul 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/1806.03790" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>We propose a new evaluation metric for information retrieval that is based on the cumulative distribution of performance across a set of queries. This metric is more robust to noise than existing metrics, and can be used to compare systems in a statistically principled way.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="jordan_distributed" class="col-sm-8"> <div class="title">Using Cumulative Distribution Based Performance Analysis to Benchmark Models</div> <div class="author"> <em>Scott M. Jordan</em>, <a href="https://dscohen.github.io/" rel="external nofollow noopener" target="_blank">Daniel Cohen</a>, and <a href="https://people.cs.umass.edu/~pthomas/" rel="external nofollow noopener" target="_blank">Philip S. Thomas</a> </div> <div class="periodical"> <em>Critiquing and Correcting Trends in Machine Learning NeurIPS Workshop</em>, Dec 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://all.cs.umass.edu/pubs/2018/Jordan%20et%20al%20-%20Distributiona%20Evaluation.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>When using only reported empirical results, it has become difficult to identify machine learning methods that provide meaningful advancement. One reason is that results are commonly only reported using well-tuned models, and thus represent an optimistic evaluation of performance. In this work, we propose a new framework for evaluating algorithms that presents both the performance when the system is well-tuned, as well as the difficulty of tuning the algorithm. This is achieved by considering the distribution of performances that result when applying the method with different hyper-parameter settings (e.g., different step sizes and network structures). Using common benchmark tasks in supervised and reinforcement learning, we demonstrate how this evaluation framework can both evaluate an algorithm’s robustness to hyper-parameter selection and identify new areas of improvement.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="ku_jordan_2017" class="col-sm-8"> <div class="title">Learning to Use a Ratchet by Modeling Spatial Relations in Demonstrations</div> <div class="author"> Li Yang Ku, <em>Scott M. Jordan</em>, Julia Badger, Erik Learned-Miller, and Rod Grupen</div> <div class="periodical"> <em>International Symposium on Experimental Robotics (ISER)</em>, Dec 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://link.springer.com/chapter/10.1007/978-3-030-33950-0_35" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://people.cs.umass.edu/%C2%A0elm/papers/RSSWLfD18.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>We introduce a framework where visual features, describing the interaction among a robot hand, a tool, and an assembly fixture, can be learned efficiently using a small number of demonstrations. We illustrate the approach by torquing a bolt with the Robonaut-2 humanoid robot using a handheld ratchet. The difficulties include the uncertainty of the ratchet pose after grasping and the high precision required for mating the socket to the bolt and replacing the tool in the tool holder. Our approach learns the desired relative position between visual features on the ratchet and the bolt. It does this by identifying goal offsets from visual features that are consistently observable over a set of demonstrations. With this approach we show that Robonaut-2 is capable of grasping the ratchet, tightening a bolt, and putting the ratchet back into a tool holder. We measure the accuracy of the socket-bolt mating subtask over multiple demonstrations and show that a small set of demonstrations can decrease the error significantly.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Scott M. Jordan. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>